<!DOCTYPE html>
<html>
<head>
  <link href='https://fonts.googleapis.com/css?family=Roboto:400,700' rel='stylesheet' type='text/css'>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet" href="https://skiadas.github.io/css/course.css" type="text/css" />
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>
<h1 id="linear-models-and-regression">Linear Models and Regression</h1>
<h2 id="reading">Reading</h2>
<p>Sections 7.1-7.3</p>
<h2 id="practice-problems">Practice Problems</h2>
<dl>
<dt>7.5.1 (Page 356)</dt>
<dd>7.1, 7.2, 7.3, 7.4
</dd>
<dt>7.5.2 (Page 362)</dt>
<dd>7.19, 7.21, 7.22, 7.23, 7.26, 7.29, 7.30, 7.31, 7.32
</dd>
</dl>
<h2 id="notes">Notes</h2>
<h3 id="basics-of-regression-lines">Basics of Regression Lines</h3>
<p>When the data appears to have an overall linear direction, it would be reasonable to attempt to obtain a linear model fit, so an equation of the form:</p>
<p><span class="math display">\[y \sim a + b x\]</span></p>
<p>where <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span> are the two parameters to be determined.</p>
<p>Notice that unlike what you may be used to, we use <span class="math inline">\(b\)</span> to denote the slope of the line, and <span class="math inline">\(a\)</span> to denote the <span class="math inline">\(y\)</span> intercept.</p>
<blockquote>
<p>The <strong>least squares regression line</strong> is the linear equation with the smallest sum of squared residuals (SSR).</p>
<p>It is obtained by computing <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span> according to the formula:</p>
<p><span class="math display">\[b = r\frac{s_y}{s_x}\]</span></p>
<p><span class="math display">\[a = \bar y - b \bar x\]</span></p>
</blockquote>
<p>Let us see this in action in the example of the city mileage and the highway mileage:</p>
<div class="figure">
<img src="images/regression.png" title="A regression line" alt="A regression line" />
<p class="caption">A regression line</p>
</div>
<p>There is overall a linear direction, and therefore it makes sense to look for a linear equation like that. Let’s see how we would compute the equation for that line by hand. We will need to know the means and standard deviations for both CMpG (our x variable) and HMpG (our y variable):</p>
<table style="width:54%;">
<colgroup>
<col width="13%" />
<col width="9%" />
<col width="13%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="right">Mean</th>
<th align="right">Std. Dev</th>
<th align="right">Correlation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">CMpG HMpG</td>
<td align="right">20.09 26.90</td>
<td align="right">5.213 5.967</td>
<td align="right">0.94</td>
</tr>
</tbody>
</table>
<p>It is really important to not forget which variable is your <span class="math inline">\(x\)</span> and which is your <span class="math inline">\(y\)</span>, or you’ll get the formulas all wrong. In our case, CMpG is our <span class="math inline">\(x\)</span>.</p>
<p>So let us compute the regression line. Slope first:</p>
<p><span class="math display">\[b = r \frac{s_y}{s_x} = 0.94\times \frac{5.967}{5.213} = 1.076\]</span></p>
<p><span class="math display">\[a = \bar y - b \bar x = 26.9 - 1.076\times 20.09 = 5.283\]</span></p>
<p>So our final equation for the regression line is:</p>
<blockquote>
<p><span class="math display">\[\hat y = 5.283 + 1.076 \times x\]</span></p>
</blockquote>
<p>We used <span class="math inline">\(\hat y\)</span> there instead of <span class="math inline">\(y\)</span> because that equation gives us the <em>predicted</em> values, not the actual <span class="math inline">\(y\)</span> values in the data.</p>
<p>Let’s use this line to do some prediction. For instance, suppose we have a car that has CMpG of 20. We would then predict that its HMpG would be:</p>
<p><span class="math display">\[5.283 + 1.076 \times 20 = 26.803\]</span></p>
<p>so we would predict a highway mileage of <span class="math inline">\(26.8\)</span> for such a car. Now if you look at the graph, you will see that there a number of cars with CMpG of 20, whose corresponding HMpGs range from around 24 to 30. There is no way for our model to predict all those accurately: Our model can only make one prediction from the CMpG of 20. So it’s bound to make some errors. But it’s doing as best as we could expect it to.</p>
<h3 id="interpreting-r-squared">Interpreting r-squared</h3>
<p>There is a certain interpretation afforded to the square of the correlation, <span class="math inline">\(r^2\)</span>. In order to understand it, we have to understand the main goal of modeling.</p>
<p>When we use a linear equation <span class="math inline">\(y=a+bx\)</span> as a model, we are in effect saying: “We know <span class="math inline">\(y\)</span> is changing, and we believe it is because <span class="math inline">\(x\)</span> is changing, and this formula tells us about this change.”</p>
<p>We can think of that equation as “explaining part of the variance in <span class="math inline">\(y\)</span>”. It does it via the predicted values <span class="math inline">\(\hat y\)</span>. Since <span class="math inline">\(\hat y\)</span> is exactly equal to <span class="math inline">\(a+bx\)</span>, all the variation that <span class="math inline">\(\hat y\)</span> undergoes is directly caused by the corresponding variation in <span class="math inline">\(x\)</span>. This is where <span class="math inline">\(r^2\)</span> comes in:</p>
<blockquote>
<p><span class="math display">\[r^2 = \frac{\textrm{Variance}(\hat y)}{\textrm{Variance}(y)}\]</span></p>
<p><span class="math inline">\(r^2\)</span> measures the percent of the variance of <span class="math inline">\(y\)</span> that is explained by the variance in <span class="math inline">\(x\)</span>.</p>
</blockquote>
<p>In our example, <span class="math inline">\(r=0.94\)</span> and so <span class="math inline">\(r^2=0.8836\)</span>. So we can say that 88% of the variation we observe in HMpG can be explained by the corresponding variation in $CMpG.</p>
<p>In fact something further happens: The overall Variance is decomposed in two parts:</p>
<blockquote>
<p><span class="math display">\[\textrm{Variance}(y) = \textrm{Variance}(\hat y) + \textrm{adjusted SSR}\]</span></p>
</blockquote>
<p>Quite literally, the overall variance is a sum of the variance of the predicted values, plus the adjusted square error, which we can think of as the leftover variance that we have not yet explained via the linear relation.</p>
<h3 id="residuals-in-the-graph">Residuals in the Graph</h3>
<p>Recall that the residual is the difference between the <span class="math inline">\(y\)</span> value and the corresponding <span class="math inline">\(\hat y\)</span> predicted value. Geometrically we can think of the residuals as the vertical distances from each point to the regression line:</p>
<div class="figure">
<img src="images/regression2.png" title="Residuals" alt="The residuals" />
<p class="caption">The residuals</p>
</div>
<p>Think of the residuals as springs attached to the line. The further they are, the more stretched out the springs, and the stronger they pull the line towards them:</p>
<blockquote>
<p>The goal of the least squares regression line is to be as close as it can to its points, resulting in small residuals.</p>
<p>The line will try to move towards points with large residuals, if it can do so without sacrificing too much on the other points.</p>
</blockquote>
<p>Here is a <a href="http://vault.hanover.edu/~skiadas/D3Regression/index2.html">little applet to play with</a> to investigate the effect of residuals on a page:</p>
<h3 id="behavior-of-outliers">Behavior of Outliers</h3>
<p>There are two kinds of outliers:</p>
<blockquote>
<p>Outliers that are far in the <span class="math inline">\(x\)</span> direction (possibly also the <span class="math inline">\(y\)</span> direction) will affect both the slope and the intercept of the line (line changes slope to move towards them).</p>
<p>Outliers that are far only in the <span class="math inline">\(y\)</span> direction will not affect the slope much, but will affect the intercept (line moves parallel up or down towards them).</p>
</blockquote>
<p>Outliers that affect the slope are usually called <em>influential</em>.</p>
<h3 id="residual-plots">Residual Plots</h3>
<p>In a residual plot, we draw a graph of the residuals on the <span class="math inline">\(y\)</span> axis, against either the <span class="math inline">\(x\)</span> or the predicted <span class="math inline">\(\hat y\)</span> values, or occationally against other variable’s values.</p>
<p>The effect of this is that it accenuates any non-linear patterns that were possibly not visible because of the dominance of the linear effect.</p>
<blockquote>
<p>In a residual plot, we look for the <em>absence of a pattern</em>. Since the residuals are the error our model makes when predicting values, they should appear to be just random “noise” if our model accurately captures the interaction between the variables.</p>
<p>If a pattern exists in the residual plot, it is an indication that our model does not accurately capture the behavior of the data, that there is something else going on.</p>
</blockquote>
<p>Here is an example of such a situation. The data does look extremely linear, and a linear model would be a good fit. In fact the correlation coefficient is <span class="math inline">\(r=0.9992\)</span>, indicating an extremely strong linear relation. The residual plot on the right however, shows that the residuals have a distinct curve to them. This is because the data actually follows a slightly quadratic pattern, that we cannot see because it is imperceptible. When we take the linear part out of the picture, the need for the quadratic term becomes clear.</p>
<div class="figure">
<img src="images/residualPlots.png" title="Residual plot" alt="Residual plot" />
<p class="caption">Residual plot</p>
</div>
<p>Studying these models goes beyond the introductory nature of this course.</p>
</body>
</html>
